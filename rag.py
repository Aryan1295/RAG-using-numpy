# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yajWsBmEnSM-KRhzeENVN5ixpjhg6OHw
"""

!pip install -q sentence-transformers #embedding model
!pip install -q wikipedia-api #Knowledge Base
!pip install -q numpy #Building vectorDB
!pip install together #LLM model

from google.colab import userdata
import os

import os
from IPython.display import Markdown
from together import Together

from together import Together

client = Together()

# response = client.chat.completions.create(
#         model="meta-llama/Llama-3.3-70B-Instruct-Turbo",
#         messages=[{"role": "user", "content": '''For the topic: Computer Vision with Generative Models and the
#         sections:
#         Generative Models in Computer Vision": "Overview of generative models, their applications in image synthesis, enhancement, and transformations,
#         Introduction to GANs: Introduction and real world applications of Generative Adversarial Networks,
#         Autoencoders & VAEs: Introduction and real world applications ,
#         Diffusion Models: Introduction and real world applications of diffusion models .

#         Give me introduction and real-world examples in details in layman terms. And don't explain your reasoning. Give some reference links as well
#         '''}],
#     )
# print(response.choices[0].message.content)

"""##Fetch Text Content from Wikipedia:"""

from wikipediaapi import Wikipedia
wiki = Wikipedia('RAGBot/0.0', 'en')
doc = wiki.page('Hayao_Miyazaki').text
paragraphs = doc.split('\n\n') #chunking

paragraphs

import textwrap

for i,p in enumerate(paragraphs):#Chuks created
  wrapped_text = textwrap.fill(p,width=100)

  print("-----------------------------------------------------------------------")
  print(wrapped_text)
  print("-----------------------------------------------------------------------")

"""##Embedding Model"""

from sentence_transformers import SentenceTransformer
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2", trust_remote_code=True)

"""##Embedd the documents"""

docs_embed = model.encode(paragraphs, normalize_embedding=True)

docs_embed.shape

docs_embed[0]

"""##Embed the query"""

query = "What was Studio Ghibli's first film?"
query_embed = model.encode(query, normalize_embedding=True)

query_embed.shape

"""##Find the closest Para in Query"""

import numpy as np
similarities = np.dot(docs_embed, query_embed.T)

similarities.shape

similarities

top_3 = np.argsort(similarities)[-3:][::-1].tolist()

top_3

most_similar_docs = [paragraphs[idx] for idx in top_3]

CONTEXT = ""
for i, p in enumerate(most_similar_docs):
  wrapped_text += textwrap.fill(p, width=100)

  print("----------------------------------")
  print(wrapped_text)
  print("----------------------------------")
  CONTEXT += wrapped_text + "\n\n"

query = "What was Studio Ghibli's first film and who was it's director?"

prompt = f'''
use the following CONTEXT to answer the QUESTION at the end.
If you don't know , just say that you don't know, don't try to make up answer.

CONTEXT : {CONTEXT}
QUESTION : {query}
'''

response = client.chat.completions.create(
        model="meta-llama/Llama-3.3-70B-Instruct-Turbo",
        messages=[{"role": "user", "content": prompt}]
    )

print(response.choices[0].message.content)